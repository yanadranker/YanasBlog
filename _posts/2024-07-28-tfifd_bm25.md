---
layout: post
title: "Ranking: TF-IDF and BM25"
date: 2024-07-28
usemath: true
---
$$
\newcommand{\tf}{\text{tf}}
\newcommand{\idf}{\text{idf}}
\newcommand{\df}{\text{df}}
\newcommand{\tfidf}{\text{tf-idf}}
$$
TF-IDF measures the importance of a word to a document in a corpus of documents. It is used as a ranking function - given a query and a corpus of documents, the documents are ranked by summing the scores of the query words. 
For a corpus $D={d_1, d_2, ..., d_n}$ of n documents and a query $q={t_1, t_2, ... t_m}$ of m terms, the score of a document $d_k, k \in {1, ..., n}$ is: 
$$\tfidf = \sum_{i \in \{1, ..., m\}} tf \text{_} idf(t_i, d_k)$$.
How can we estimate a word importance in a specific document? To mathematically quantify this we make two assumptions:
1. frequency -> relevance.
As there are more appearances of a term in a document, it is more likely that the document is about that term. Therefore, more appearances should increase the score. We define the tf (term frequency) as:
$$tf(t, d) = \frac{count(t, d)}{\sum_i count(t_i, d)}$$
where $count(t, d)$ is the number of times the term t appears in document d.
3. rarity -> significance.
There are certain words that are very frequent in the language, regardless of the subject, for example articles ("the", "a"), prepositions ("to", "of", "in"), conjunction ("and"). For such words, a frequent appearnace does not hold semantic meaning. They tend to appear in every text equally often. Therefore a word that appears often on all texts should decrease the score. We define the df (document frequency) term:
$$df(t) = \frac{doc \text{_} count(t)}{n}$$
where $doc \text{_} count(t)$ is the number of documents in which the term t appears.

So a possible formulation of tf-idf is:
$$\tfidf = tf(t, d) / df(t) = \frac{\frac{count(t, d)}{\sum_i count(t_i, d)}}{\frac{doc \text{_} count(t)}{n}} = \frac{count(t, d)}{\sum_i count(t_i, d)} * \frac{n}{doc \text{_} count(t)} = tf(t, d) * idf(t)$$
There is still another heuristic that needs to be addressed for the final score. For large corpuses (i.e. large n), small differences in document count $doc \text{_} count(t)$ incur large penalties. For example, for $n=100$, if a term appears in 1 document $\text{idf} = \frac{100}{1} = 100$, and if a term appears in 2 documents $\text{idf} = \frac{100}{2} = 50$. To smooth out this effect a log is added to the idf term. 

<img src="{{ site.github.url }}/assets/img/tfidf/log_idf.png" alt="Smoothing the idf term.">

The final tf-idf score is:
$$\tfidf = \frac{count(t, d)}{\sum_i count(t_i, d)} * log(\frac{n}{doc \text{_} count(t)})$$.

Side notes:
* A similar claim can be made for adding log to the tf term. Indeed, there are sevral variants of tf-idf, one of them is log scaled tf.
* Connection to information theory?


### References
[UNDERSTANDING TF-IDF AND BM-25](https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/)

[Wikipedia: tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#)
