---
layout: post
title: "Ranking: TF-IDF and BM25"
date: 2024-07-28
usemath: true
---
TF-IDF measures the importance of a word to a document in a corpus of documents. It is used as a ranking function - given a query and a corpus of documents, the documents are ranked by summing the scores of the query words. 
For a corpus $D={d_1, d_2, ..., d_n}$ of n documents and a query $q={t_1, t_2, ... t_m}$ of m terms, the score of a document $d_k, k \in {1, ..., n}$ is: $\sum_{i \in \{1, ..., m\}} tf-idf(t_i, d_k)$.
How do we know if a word is important in a specific document? To mathematically quantify this we make two assumptions:
1. frequency -> relevance.
As there are more appearances of a term in a document, it is more likely that the document is about that term. Therefore, more appearances should increase the score.
$$tf(t, d) = \frac{count(t, d), \sum_i count(t_i, d)}$$
where $count(t, d)$ is the number of times the term t appears in document d.
3. rarity -> significance.
There are certain words that are very frequent in the language, regardless of the subject, for example articles ("the", "a"), prepositions ("to", "of", "in"), conjunction ("and"). For such words, a frequent appearnace does not hold semantic meaning. They tend to appear in every text equally often. Therefore a word that appears often on all texts should decrease the score.
$$df(t) = \frac{doc_count(t), n}$$
where $doc_count(t)$ is the number of documents in which the term t appears.

So a possible formulation of tf-idf is:
$$tf(t, d) / df(t) = \frac{\frac{count(t, d), \sum_i count(t_i, d)},\frac{doc_count(t), n}} = \frac{count(t, d), \sum_i count(t_i, d)} * \frac{n , doc_count(t)} = tf(t, d) * idf(t)$$
However, there is still another heuristic that needs to be addressed for the final score. For large corpuses (i.e. large n), small differences in document count $doc_count(t)$ incur large penalties. For example, for $n=100$, if a term appears in 1 document $idf = \frac{100, 1} = 100$, and if a term appears in 2 documents $idf = \frac{100, 2} = 50$. To smooth out this effect a log is added to the idf term. 
![alt text](tfidf/log_idf.png "Smoothing idf term")
The final tf-idf score is:
$$\frac{count(t, d), \sum_i count(t_i, d)} * log(\frac{n , doc_count(t)})$$.

Side notes:
* A similar claim can be made for adding log to the tf term. Indeed, there are sevral variants of tf-idf, one of them is log scaled tf.
* Connection to information theory?

  


### References
[UNDERSTANDING TF-IDF AND BM-25](https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/)
[Wikipedia: tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#)
